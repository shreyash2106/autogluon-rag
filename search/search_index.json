{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoGluon-RAG Github Repo AutoGluon-RAG is a framework designed to streamline the development of RAG (Retrieval-Augmented Generation) pipelines. RAG has emerged as a crucial approach for tailoring large language models (LLMs) to address domain-specific queries. However, constructing RAG pipelines traditionally involves navigating through a complex array of modules and functionalities, including retrievers, generators, vector database construction, fast semantic search, and handling long-context inputs, among others. AutoGluon-RAG allows users to create customized RAG pipelines seamlessly, eliminating the need to delve into any technical complexities. Following the AutoML (Automated Machine Learning) philosophy of simplifying model development with minimal code, as exemplified by AutoGluon; AutoGluon-RAG enables users to create a RAG pipeline with just a few lines of code. The framework provides a user-friendly interface, and abstracts away the underlying modules, allowing users to focus on their domain-specific requirements and leveraging the power of RAG pipelines without the need for extensive technical expertise. Goal In line with the AutoGluon team's commitment to meeting user requirements and expanding its user base, the team aims to develop a new feature that simplifies the creation and deployment of end-to-end RAG (Retrieval-Augmented Generation) pipelines. Given a set of user-provided data or documents, this feature will enable users to develop and deploy a RAG pipeline with minimal coding effort, following the AutoML (Automated Machine Learning) philosophy of three-line solutions.","title":"Home"},{"location":"#autogluon-rag","text":"Github Repo AutoGluon-RAG is a framework designed to streamline the development of RAG (Retrieval-Augmented Generation) pipelines. RAG has emerged as a crucial approach for tailoring large language models (LLMs) to address domain-specific queries. However, constructing RAG pipelines traditionally involves navigating through a complex array of modules and functionalities, including retrievers, generators, vector database construction, fast semantic search, and handling long-context inputs, among others. AutoGluon-RAG allows users to create customized RAG pipelines seamlessly, eliminating the need to delve into any technical complexities. Following the AutoML (Automated Machine Learning) philosophy of simplifying model development with minimal code, as exemplified by AutoGluon; AutoGluon-RAG enables users to create a RAG pipeline with just a few lines of code. The framework provides a user-friendly interface, and abstracts away the underlying modules, allowing users to focus on their domain-specific requirements and leveraging the power of RAG pipelines without the need for extensive technical expertise.","title":"AutoGluon-RAG"},{"location":"#goal","text":"In line with the AutoGluon team's commitment to meeting user requirements and expanding its user base, the team aims to develop a new feature that simplifies the creation and deployment of end-to-end RAG (Retrieval-Augmented Generation) pipelines. Given a set of user-provided data or documents, this feature will enable users to develop and deploy a RAG pipeline with minimal coding effort, following the AutoML (Automated Machine Learning) philosophy of three-line solutions.","title":"Goal"},{"location":"common_errors/","text":"Below is a list of common errors in AutoGluon-RAG 1. Shape Mismatch / Shape Errors A common error you may encounter when setting up the RAG pipeline end-to-end is shape errors when creating embeddings or storing them in the database. Here are some quick fixes to this problem: 1. Pooling for embedding model: Some models may require pooling the output to obtain the actual embeddings. Make sure you check the documentation of the model you are using to decide what pooling is required. 2. Document/Website access: Make sure the documents and/or websites you provide are accessible to the package. The provided files may be blocked at a system-level and provided websites may not be accessible by web scrapers. This could lead to no data being processed and empty embedding arrays. 2. Model Access Issues Refer to the tutorial Accessing models through different services for more information about model access setup. These refer to both embedding models and generative models. 3. AWS Resource Issues Refer to the tutorial Using AWS resources and services for more information about AWS access setup.","title":"Below is a list of common errors in AutoGluon-RAG"},{"location":"common_errors/#below-is-a-list-of-common-errors-in-autogluon-rag","text":"","title":"Below is a list of common errors in AutoGluon-RAG"},{"location":"common_errors/#1-shape-mismatch-shape-errors","text":"A common error you may encounter when setting up the RAG pipeline end-to-end is shape errors when creating embeddings or storing them in the database. Here are some quick fixes to this problem: 1. Pooling for embedding model: Some models may require pooling the output to obtain the actual embeddings. Make sure you check the documentation of the model you are using to decide what pooling is required. 2. Document/Website access: Make sure the documents and/or websites you provide are accessible to the package. The provided files may be blocked at a system-level and provided websites may not be accessible by web scrapers. This could lead to no data being processed and empty embedding arrays.","title":"1. Shape Mismatch / Shape Errors"},{"location":"common_errors/#2-model-access-issues","text":"Refer to the tutorial Accessing models through different services for more information about model access setup. These refer to both embedding models and generative models.","title":"2. Model Access Issues"},{"location":"common_errors/#3-aws-resource-issues","text":"Refer to the tutorial Using AWS resources and services for more information about AWS access setup.","title":"3. AWS Resource Issues"},{"location":"evaluation/","text":"AutoGluon-RAG Evaluation Module Overview The EvaluationModule in AutoGluon-RAG is designed to facilitate the evaluation of the Retrieval-Augmented Generation (RAG) pipeline. This module allows users to easily assess the performance of the RAG pipeline using various datasets and evaluation metrics. The primary purpose of this module is to provide a flexible and extensible framework for evaluating the quality and effectiveness of the generated responses by the RAG pipeline. The EvaluationModule is created to: Simplify the evaluation process for the AutoGluon-RAG pipeline. Support multiple evaluation metrics, including custom metrics. Provide functionality to save evaluation data and results. Allow users to preprocess data, extract queries, and expected responses from various datasets. Usage Initialization To initialize the EvaluationModule , you need to provide an instance of AutoGluonRAG , the name of the dataset, a list of metrics, and additional optional parameters such as preprocessing functions and paths for saving evaluation data. from agrag.agrag import AutoGluonRAG from agrag.evaluation.evaluator import EvaluationModule from module import preprocessing_fn, query_fn, response_fn evaluation_dir = \"evaluation_data\" agrag = AutoGluonRAG(preset_quality=\"medium_quality\", data_dir=evaluation_dir) evaluator = EvaluationModule(rag_instance=agrag) evaluator.run_evaluation( dataset_name=\"huggingface_dataset/dataset_name\", metrics=[\"exact_match\", \"transformer_matcher\"], save_evaluation_data=True, evaluation_dir=evaluation_dir, preprocessing_fn=preprocessing_fn, query_fn=query_fn, response_fn=response_fn, hf_dataset_params={\"name\": \"dev\"}, ) Refer to src/agrag/evaluation/datasets/google_natural_questions/evaluate_agrag.py for a detailed example of how to evaluate AutoGluon-RAG on the Google Natural Questions dataset from HuggingFace. Arguments to Evaluation Module agrag Type : AutoGluonRAG Description : The AutoGluonRAG instance to be evaluated. from agrag.agrag import AutoGluonRAG agrag = AutoGluonRAG(preset_quality=\"medium_quality\", data_dir=evaluation_dir) # Calling agrag.initialize_rag_pipeline() is optional since the EvaluationModule will initialize the pipeline if it has not been done already. AutoGluon-RAG for Large Datasets For large datasets, a naive version of AutoGluon-RAG may not be sufficient. Here are some steps you can take when working with a large corpus for RAG: Using optimized indices for Vector DB. Refer to the documentation for the supported vector databases on how you can use optimized indices such as clustered and quantized databases. Set the parameters appropriately in your configuration file. For example, here is an optimized FAISS setup (in the configuration file) using quantization that runs correctly for the Google Natural Questions dataset: vector_db: db_type: faiss faiss_index_type: IndexIVFPQ faiss_quantized_index_params: nlist: 50 m: 8 bits: 8 faiss_index_nprobe: 15 Use GPUs: Make sure to use GPUs appropriately in each module (wherever applicable). You can set the num_gpus parameter in the configuration file under each module. System Memory: Make sure your system has enough RAM for at least the size of the dataset. You will require more memory for the documents that will be generated from the datasets, the embeddings, the metadata, and the vector db index. We recommend running evaluation on a remote instance (such as AWS EC2) instead of running locally. If you would like to run locally, you can choose to run a subset of the evaluation data by setting max_eval_size when calling the run_evaluation function (see the next section). Arguments to run_evaluation function NOTE Every time you run the run_evaluation function, you may need to set the agrag.data_dir parameter if you change the dataset being used. In that case, you will have to reinitialize the RAG pipeline. Alternatively, you can index all your evaluation datasets at once, or create multiple instances of AutoGluonRAG . dataset_name Type : str Description : The name of the dataset to use for evaluation. metrics Type : List[Union[str, Callable]] Description : The list of metrics to use for evaluation. Supported metrics include: \"bertscore\" : Uses the BERTScore metric from HuggingFace. \"bleu\" : Uses the BLEU metric from HuggingFace \"hf_exact_match\" Uses the Exact Match Metric from HuggingFace \"inclusive_exact_match\" : Uses the Inclusive Exact Match metric. This is a custom metric defined in this module since it is a bit more lenient compared to the HuggingFace exact_match metric. It also counts events where the expected response is contained within the generated response as a success. This metric supports the following optional parameters: regexes_to_ignore (List[str], optional): A list of regex expressions of characters to ignore when calculating the exact matches. Defaults to None . Note: the regex changes are applied before capitalization is normalized. ignore_case (bool, optional): If True , turns everything to lowercase so that capitalization differences are ignored. Defaults to False . ignore_punctuation (bool, optional): If True , removes punctuation before comparing strings. Defaults to False . ignore_numbers (bool, optional): If True , removes all digits before comparing strings. Defaults to False . \"pedant\" : Uses the PEDANT metric from QA Metrics . \"transformer_matcher\" : Uses the Transformer Matcher metric from QA Metrics . <callable_custom_metric> : Any callable Python function or a function from a Python package. It must take in at least the arguments predictions and references , where predictions is a List of generated responses and references is a List[List] of expected responses. metric_score_params Type : dict Description : Optional, additional parameters to pass into evaluation metric functions when computing scores. metric_init_params Type : dict Description : Optional, additional parameters to pass into evaluation metric functions when initializing the functions. preprocessing_fn Type : Callable Description : A function to preprocess the content before saving. This must be a function that returns the relevant content from the dataset row. For example, to extract text from the Google Natural Questions dataset, you can pass in this function as preprocessing_fn : def preprocess_google_nq(row): \"\"\" Extracts text from HTML content for the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset containing HTML content. Returns: ------- str The extracted text content. \"\"\" html_content = row[\"document\"][\"html\"] return extract_text_from_html(html_content) # Function to extract text from HTML query_fn Type : Callable Description : A function to extract the query from the dataset row. For example, to extract the query from the Google Natural Questions dataset, you can pass in this function as query_fn : def get_google_nq_query(row): \"\"\" Extracts the query from a row in the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset. Returns: ------- str The query. \"\"\" return row[\"question\"][\"text\"] response_fn Type : Callable Description : A function to extract the expected responses from the dataset row. For example, to extract the expected responses from the Google Natural Questions dataset, you can pass in this function as query_fn : def get_google_nq_responses(row): \"\"\" Extracts the expected responses from a row in the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset. Returns: ------- List[str] A list of expected responses. \"\"\" short_answers = row[\"annotations\"][\"short_answers\"] return [answer[\"text\"][0] for answer in short_answers if answer[\"text\"]] hf_dataset_params Type : dict Description : Additional parameters to pass into the HuggingFace load_dataset function. split Type : str Description : The dataset split to use (default is \"validation\"). save_evaluation_data Type : bool Description : Whether to save evaluation data to files (default is True). Set this to False if you already have a directory of evaluation files to pass into AutoGluon RAG. evaluation_dir Type : str Description : The directory for evaluation data (default is \"./evaluation_data\"). save_csv_path Type : str Description : The path to save the evaluation results as a CSV file (default is None). If no path is provided, the evaluation results will not be saved. max_eval_size Type : int , optional Description : The maximum number of datapoints to process for evaluation (default is None). If this value is not less than the total number of datapoints (rows), the entire dataset will be used. Using a Custom Dataset If you would like to use your own dataset, you must structure it with following columns: \"content\": Column containing document content (in the form of plaintext) that will be extracted and passed into the RAG pipeline \"query\": Column containing a singular query for each document that will be passed into the generator. \"expected_responses\": A list of expected responses for each query that will be used for evaluation. For the parameters preprocessing_fn , query_fn , response_fn , the EvaluationModule will use the functions in src/agrag/evaluation/dataset_utils.py by default.","title":"Evaluation"},{"location":"evaluation/#autogluon-rag-evaluation-module","text":"","title":"AutoGluon-RAG Evaluation Module"},{"location":"evaluation/#overview","text":"The EvaluationModule in AutoGluon-RAG is designed to facilitate the evaluation of the Retrieval-Augmented Generation (RAG) pipeline. This module allows users to easily assess the performance of the RAG pipeline using various datasets and evaluation metrics. The primary purpose of this module is to provide a flexible and extensible framework for evaluating the quality and effectiveness of the generated responses by the RAG pipeline. The EvaluationModule is created to: Simplify the evaluation process for the AutoGluon-RAG pipeline. Support multiple evaluation metrics, including custom metrics. Provide functionality to save evaluation data and results. Allow users to preprocess data, extract queries, and expected responses from various datasets.","title":"Overview"},{"location":"evaluation/#usage","text":"","title":"Usage"},{"location":"evaluation/#initialization","text":"To initialize the EvaluationModule , you need to provide an instance of AutoGluonRAG , the name of the dataset, a list of metrics, and additional optional parameters such as preprocessing functions and paths for saving evaluation data. from agrag.agrag import AutoGluonRAG from agrag.evaluation.evaluator import EvaluationModule from module import preprocessing_fn, query_fn, response_fn evaluation_dir = \"evaluation_data\" agrag = AutoGluonRAG(preset_quality=\"medium_quality\", data_dir=evaluation_dir) evaluator = EvaluationModule(rag_instance=agrag) evaluator.run_evaluation( dataset_name=\"huggingface_dataset/dataset_name\", metrics=[\"exact_match\", \"transformer_matcher\"], save_evaluation_data=True, evaluation_dir=evaluation_dir, preprocessing_fn=preprocessing_fn, query_fn=query_fn, response_fn=response_fn, hf_dataset_params={\"name\": \"dev\"}, ) Refer to src/agrag/evaluation/datasets/google_natural_questions/evaluate_agrag.py for a detailed example of how to evaluate AutoGluon-RAG on the Google Natural Questions dataset from HuggingFace.","title":"Initialization"},{"location":"evaluation/#arguments-to-evaluation-module","text":"","title":"Arguments to Evaluation Module"},{"location":"evaluation/#agrag","text":"Type : AutoGluonRAG Description : The AutoGluonRAG instance to be evaluated. from agrag.agrag import AutoGluonRAG agrag = AutoGluonRAG(preset_quality=\"medium_quality\", data_dir=evaluation_dir) # Calling agrag.initialize_rag_pipeline() is optional since the EvaluationModule will initialize the pipeline if it has not been done already.","title":"agrag"},{"location":"evaluation/#autogluon-rag-for-large-datasets","text":"For large datasets, a naive version of AutoGluon-RAG may not be sufficient. Here are some steps you can take when working with a large corpus for RAG: Using optimized indices for Vector DB. Refer to the documentation for the supported vector databases on how you can use optimized indices such as clustered and quantized databases. Set the parameters appropriately in your configuration file. For example, here is an optimized FAISS setup (in the configuration file) using quantization that runs correctly for the Google Natural Questions dataset: vector_db: db_type: faiss faiss_index_type: IndexIVFPQ faiss_quantized_index_params: nlist: 50 m: 8 bits: 8 faiss_index_nprobe: 15 Use GPUs: Make sure to use GPUs appropriately in each module (wherever applicable). You can set the num_gpus parameter in the configuration file under each module. System Memory: Make sure your system has enough RAM for at least the size of the dataset. You will require more memory for the documents that will be generated from the datasets, the embeddings, the metadata, and the vector db index. We recommend running evaluation on a remote instance (such as AWS EC2) instead of running locally. If you would like to run locally, you can choose to run a subset of the evaluation data by setting max_eval_size when calling the run_evaluation function (see the next section).","title":"AutoGluon-RAG for Large Datasets"},{"location":"evaluation/#arguments-to-run_evaluation-function","text":"","title":"Arguments to run_evaluation function"},{"location":"evaluation/#note","text":"Every time you run the run_evaluation function, you may need to set the agrag.data_dir parameter if you change the dataset being used. In that case, you will have to reinitialize the RAG pipeline. Alternatively, you can index all your evaluation datasets at once, or create multiple instances of AutoGluonRAG .","title":"NOTE"},{"location":"evaluation/#dataset_name","text":"Type : str Description : The name of the dataset to use for evaluation.","title":"dataset_name"},{"location":"evaluation/#metrics","text":"Type : List[Union[str, Callable]] Description : The list of metrics to use for evaluation. Supported metrics include: \"bertscore\" : Uses the BERTScore metric from HuggingFace. \"bleu\" : Uses the BLEU metric from HuggingFace \"hf_exact_match\" Uses the Exact Match Metric from HuggingFace \"inclusive_exact_match\" : Uses the Inclusive Exact Match metric. This is a custom metric defined in this module since it is a bit more lenient compared to the HuggingFace exact_match metric. It also counts events where the expected response is contained within the generated response as a success. This metric supports the following optional parameters: regexes_to_ignore (List[str], optional): A list of regex expressions of characters to ignore when calculating the exact matches. Defaults to None . Note: the regex changes are applied before capitalization is normalized. ignore_case (bool, optional): If True , turns everything to lowercase so that capitalization differences are ignored. Defaults to False . ignore_punctuation (bool, optional): If True , removes punctuation before comparing strings. Defaults to False . ignore_numbers (bool, optional): If True , removes all digits before comparing strings. Defaults to False . \"pedant\" : Uses the PEDANT metric from QA Metrics . \"transformer_matcher\" : Uses the Transformer Matcher metric from QA Metrics . <callable_custom_metric> : Any callable Python function or a function from a Python package. It must take in at least the arguments predictions and references , where predictions is a List of generated responses and references is a List[List] of expected responses.","title":"metrics"},{"location":"evaluation/#metric_score_params","text":"Type : dict Description : Optional, additional parameters to pass into evaluation metric functions when computing scores.","title":"metric_score_params"},{"location":"evaluation/#metric_init_params","text":"Type : dict Description : Optional, additional parameters to pass into evaluation metric functions when initializing the functions.","title":"metric_init_params"},{"location":"evaluation/#preprocessing_fn","text":"Type : Callable Description : A function to preprocess the content before saving. This must be a function that returns the relevant content from the dataset row. For example, to extract text from the Google Natural Questions dataset, you can pass in this function as preprocessing_fn : def preprocess_google_nq(row): \"\"\" Extracts text from HTML content for the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset containing HTML content. Returns: ------- str The extracted text content. \"\"\" html_content = row[\"document\"][\"html\"] return extract_text_from_html(html_content) # Function to extract text from HTML","title":"preprocessing_fn"},{"location":"evaluation/#query_fn","text":"Type : Callable Description : A function to extract the query from the dataset row. For example, to extract the query from the Google Natural Questions dataset, you can pass in this function as query_fn : def get_google_nq_query(row): \"\"\" Extracts the query from a row in the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset. Returns: ------- str The query. \"\"\" return row[\"question\"][\"text\"]","title":"query_fn"},{"location":"evaluation/#response_fn","text":"Type : Callable Description : A function to extract the expected responses from the dataset row. For example, to extract the expected responses from the Google Natural Questions dataset, you can pass in this function as query_fn : def get_google_nq_responses(row): \"\"\" Extracts the expected responses from a row in the Google NQ dataset. Parameters: ---------- row : dict A row from the Google NQ dataset. Returns: ------- List[str] A list of expected responses. \"\"\" short_answers = row[\"annotations\"][\"short_answers\"] return [answer[\"text\"][0] for answer in short_answers if answer[\"text\"]]","title":"response_fn"},{"location":"evaluation/#hf_dataset_params","text":"Type : dict Description : Additional parameters to pass into the HuggingFace load_dataset function.","title":"hf_dataset_params"},{"location":"evaluation/#split","text":"Type : str Description : The dataset split to use (default is \"validation\").","title":"split"},{"location":"evaluation/#save_evaluation_data","text":"Type : bool Description : Whether to save evaluation data to files (default is True). Set this to False if you already have a directory of evaluation files to pass into AutoGluon RAG.","title":"save_evaluation_data"},{"location":"evaluation/#evaluation_dir","text":"Type : str Description : The directory for evaluation data (default is \"./evaluation_data\").","title":"evaluation_dir"},{"location":"evaluation/#save_csv_path","text":"Type : str Description : The path to save the evaluation results as a CSV file (default is None). If no path is provided, the evaluation results will not be saved.","title":"save_csv_path"},{"location":"evaluation/#max_eval_size","text":"Type : int , optional Description : The maximum number of datapoints to process for evaluation (default is None). If this value is not less than the total number of datapoints (rows), the entire dataset will be used.","title":"max_eval_size"},{"location":"evaluation/#using-a-custom-dataset","text":"If you would like to use your own dataset, you must structure it with following columns: \"content\": Column containing document content (in the form of plaintext) that will be extracted and passed into the RAG pipeline \"query\": Column containing a singular query for each document that will be passed into the generator. \"expected_responses\": A list of expected responses for each query that will be used for evaluation. For the parameters preprocessing_fn , query_fn , response_fn , the EvaluationModule will use the functions in src/agrag/evaluation/dataset_utils.py by default.","title":"Using a Custom Dataset"},{"location":"tutorial/","text":"Below is a list of tutorials with different use-cases for AutoGluon-RAG General Setting Parameters for AutoGluonRAG through code Accessing models through different services Data Processing Module Using AutoGluon-RAG to obtain processed data from documents/websites . This will include data in the form of text chunks for each document/website in the directory. Embedding Module Using AutoGluon-RAG to generate embeddings Vector DB Module Optimizing FAISS for large-scale data Retriever Module Disable the reranker and only use embedding model for retrieval Generator Module Change generator after initializing RAG pipeline","title":"Tutorials"},{"location":"tutorial/#below-is-a-list-of-tutorials-with-different-use-cases-for-autogluon-rag","text":"","title":"Below is a list of tutorials with different use-cases for AutoGluon-RAG"},{"location":"tutorial/#general","text":"Setting Parameters for AutoGluonRAG through code Accessing models through different services","title":"General"},{"location":"tutorial/#data-processing-module","text":"Using AutoGluon-RAG to obtain processed data from documents/websites . This will include data in the form of text chunks for each document/website in the directory.","title":"Data Processing Module"},{"location":"tutorial/#embedding-module","text":"Using AutoGluon-RAG to generate embeddings","title":"Embedding Module"},{"location":"tutorial/#vector-db-module","text":"Optimizing FAISS for large-scale data","title":"Vector DB Module"},{"location":"tutorial/#retriever-module","text":"Disable the reranker and only use embedding model for retrieval","title":"Retriever Module"},{"location":"tutorial/#generator-module","text":"Change generator after initializing RAG pipeline","title":"Generator Module"},{"location":"usage/","text":"Usage To use this framework, you must first install AutoGluon RAG: git clone https://github.com/autogluon/autogluon-rag cd autogluon-rag # Create a Virtual Environment (using Python, or conda if you prefer) python3 -m virtualenv venv source venv/bin/activate #Install the package pip install -e . You can now use the package in two ways. Use AutoGluon-RAG through the command line as agrag : AutoGluon-RAG usage: agrag [-h] --config_file AutoGluon-RAG - Retrieval-Augmented Generation Pipeline options: -h, --help show this help message and exit --config_file Path to the configuration file Use AutoGluon-RAG through code: from agrag.agrag import AutoGluonRAG def ag_rag(): agrag = AutoGluonRAG( preset_quality=\"medium_quality\", # or path to config file web_urls=[\"https://auto.gluon.ai/stable/index.html\"], base_urls=[\"https://auto.gluon.ai/stable/\"], parse_urls_recursive=True, data_dir=\"s3://autogluon-rag-github-dev/autogluon_docs/\" ) agrag.initialize_rag_pipeline() agrag.generate_response(\"What is AutoGluon?\") if __name__ == \"__main__\": ag_rag() Configuring Parameters for AutoGluon-RAG: Using AutoGluonRAG class For a list of configurable parameters that can be passed into the AutoGluonRAG class, refer to the tutorial here . Using Configuration File You can also use a configuration file with AutoGluonRAG . The configuration file contains the specific parameters to use for each module in the RAG pipeline. For an example of a config file, please refer to example_config.yaml in src/agrag/configs/ . For specific details about the parameters in each individual module, refer to the README files in each module in src/agrag/modules/ . There is also a shared section in the config file for parameters that do not refer to a specific module. Currently, the parameters in shared are: pipeline_batch_size: Optional batch size to use for pre-processing stage (Data Processing, Embedding, Vector DB Module). This represents the number of files in each batch. The default value is 20.","title":"Usage"},{"location":"usage/#usage","text":"To use this framework, you must first install AutoGluon RAG: git clone https://github.com/autogluon/autogluon-rag cd autogluon-rag # Create a Virtual Environment (using Python, or conda if you prefer) python3 -m virtualenv venv source venv/bin/activate #Install the package pip install -e . You can now use the package in two ways.","title":"Usage"},{"location":"usage/#use-autogluon-rag-through-the-command-line-as-agrag","text":"AutoGluon-RAG usage: agrag [-h] --config_file AutoGluon-RAG - Retrieval-Augmented Generation Pipeline options: -h, --help show this help message and exit --config_file Path to the configuration file","title":"Use AutoGluon-RAG through the command line as agrag:"},{"location":"usage/#use-autogluon-rag-through-code","text":"from agrag.agrag import AutoGluonRAG def ag_rag(): agrag = AutoGluonRAG( preset_quality=\"medium_quality\", # or path to config file web_urls=[\"https://auto.gluon.ai/stable/index.html\"], base_urls=[\"https://auto.gluon.ai/stable/\"], parse_urls_recursive=True, data_dir=\"s3://autogluon-rag-github-dev/autogluon_docs/\" ) agrag.initialize_rag_pipeline() agrag.generate_response(\"What is AutoGluon?\") if __name__ == \"__main__\": ag_rag()","title":"Use AutoGluon-RAG through code:"},{"location":"usage/#configuring-parameters-for-autogluon-rag","text":"","title":"Configuring Parameters for AutoGluon-RAG:"},{"location":"usage/#using-autogluonrag-class","text":"For a list of configurable parameters that can be passed into the AutoGluonRAG class, refer to the tutorial here .","title":"Using AutoGluonRAG class"},{"location":"usage/#using-configuration-file","text":"You can also use a configuration file with AutoGluonRAG . The configuration file contains the specific parameters to use for each module in the RAG pipeline. For an example of a config file, please refer to example_config.yaml in src/agrag/configs/ . For specific details about the parameters in each individual module, refer to the README files in each module in src/agrag/modules/ . There is also a shared section in the config file for parameters that do not refer to a specific module. Currently, the parameters in shared are: pipeline_batch_size: Optional batch size to use for pre-processing stage (Data Processing, Embedding, Vector DB Module). This represents the number of files in each batch. The default value is 20.","title":"Using Configuration File"},{"location":"tutorials/data_processing/process_data/","text":"This is a tutorial on using AutoGluon-RAG to process data from documents/websites. agrag = AutoGluonRAG( data_dir=\"path/to/data\", preset_quality=\"medium_quality\", # or path to config file ) agrag.initialize_data_module() processed_data = self.process_data() Here, instead of calling initialize_rag_pipeline to initialize the entire pipeline, we simply initialize the data module to process the data. process_data returns a pandas DataFrame with the following columns: \"doc_id\", \"chunk_id\", \"text\" . You can obtain the actual text by: text_list = processed_data[\"text\"].tolist() text_array = np.array(text_list)","title":"Process data"},{"location":"tutorials/data_processing/process_data/#this-is-a-tutorial-on-using-autogluon-rag-to-process-data-from-documentswebsites","text":"agrag = AutoGluonRAG( data_dir=\"path/to/data\", preset_quality=\"medium_quality\", # or path to config file ) agrag.initialize_data_module() processed_data = self.process_data() Here, instead of calling initialize_rag_pipeline to initialize the entire pipeline, we simply initialize the data module to process the data. process_data returns a pandas DataFrame with the following columns: \"doc_id\", \"chunk_id\", \"text\" . You can obtain the actual text by: text_list = processed_data[\"text\"].tolist() text_array = np.array(text_list)","title":"This is a tutorial on using AutoGluon-RAG to process data from documents/websites."},{"location":"tutorials/embedding/generate_embeddings/","text":"This is a tutorial on using AutoGluon-RAG to generate embeddings. agrag = AutoGluonRAG( data_dir=\"path/to/data\", preset_quality=\"medium_quality\", # or path to config file ) agrag.initialize_data_module() agrag.initialize_embedding_module() processed_data = self.process_data() embeddings = agrag.generate_embeddings(processed_data=processed_data) Here, instead of calling initialize_rag_pipeline to initialize the entire pipeline, we simply initialize the data and embedding modules to generate the embeddings. generate_embeddings returns a pandas DataFrame with the following columns: \"doc_id\", \"chunk_id\", \"text\", \"embedding\", \"all_embeddings_hidden_dim\" . You can obtain the actual embeddings by: embeddings_list = embeddings[\"embedding\"].tolist() embeddings_array = np.array(embeddings_list)","title":"Generate embeddings"},{"location":"tutorials/embedding/generate_embeddings/#this-is-a-tutorial-on-using-autogluon-rag-to-generate-embeddings","text":"agrag = AutoGluonRAG( data_dir=\"path/to/data\", preset_quality=\"medium_quality\", # or path to config file ) agrag.initialize_data_module() agrag.initialize_embedding_module() processed_data = self.process_data() embeddings = agrag.generate_embeddings(processed_data=processed_data) Here, instead of calling initialize_rag_pipeline to initialize the entire pipeline, we simply initialize the data and embedding modules to generate the embeddings. generate_embeddings returns a pandas DataFrame with the following columns: \"doc_id\", \"chunk_id\", \"text\", \"embedding\", \"all_embeddings_hidden_dim\" . You can obtain the actual embeddings by: embeddings_list = embeddings[\"embedding\"].tolist() embeddings_array = np.array(embeddings_list)","title":"This is a tutorial on using AutoGluon-RAG to generate embeddings."},{"location":"tutorials/general/aws_resources/","text":"This is a tutorial on using AWS resources and services in AutoGluonRAG . To use any AWS resources, you need to make sure you have an appropriate AWS account with access to the required resources. Additionally, you will need to set up your access credentials. You can either use the AWS CLI or manually set the AWS Keys in your command line configuration file ( bash_profile or zshrc file). If you are doing it manually, make sure to set the following parameters: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY - AWS_DEFAULT_REGION","title":"Aws resources"},{"location":"tutorials/general/aws_resources/#this-is-a-tutorial-on-using-aws-resources-and-services-in-autogluonrag","text":"To use any AWS resources, you need to make sure you have an appropriate AWS account with access to the required resources. Additionally, you will need to set up your access credentials. You can either use the AWS CLI or manually set the AWS Keys in your command line configuration file ( bash_profile or zshrc file). If you are doing it manually, make sure to set the following parameters: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY - AWS_DEFAULT_REGION","title":"This is a tutorial on using AWS resources and services in AutoGluonRAG."},{"location":"tutorials/general/code_parameters/","text":"This is a tutorial on the optional parameters that can be passed into the AutoGluonRAG class when using the package through code. These are the optional parameters that can be passed into the AutoGluonRAG class: config_file : str Path to a configuration file that will be used to set specific parameters in the RAG pipeline. preset_quality : str If you do not wish to use your own configuration file, you can use a preset configuration file which contains pre-defined arguments. You must provide the preset quality setting (\"good_quality\", \"medium_quality\", or, \"best_quality\"). Note that if both config_file and preset_quality are provided, config_file will be prioritized. model_ids : dict Dictionary of model IDs to use for specific modules. Example: {\"generator_model_id\": \"mistral.mistral-7b-instruct-v0:2\", \"retriever_model_id\": \"BAAI/bge-large-en\", \"reranker_model_id\": \"nv_embed\"} data_dir : str The directory containing the data files that will be used for the RAG pipeline. If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. web_urls : List[str] List of website URLs to be ingested and processed. Each URL will processed recursively based on the base URL to include the content of URLs that exist within this URL. If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. base_urls : List[str] List of optional base URLs to check for links recursively. The base URL controls which URLs will be processed during recursion. The base_url does not need to be the same as the web_url. For example. the web_url can be \"https://auto.gluon.ai/stable/index.html\", and the base_urls will be \"https://auto.gluon.ai/stable/\". If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. login_info: dict A dictionary containing login credentials for each URL. Required if the target URL requires authentication. Must be structured as {target_url: {\"login_url\": <login_url>, \"credentials\": {\"username\": \"your_username\", \"password\": \"your_password\"}}} The target_url is a url that is present in the list of web_urls parse_urls_recursive: bool Whether to parse each URL in the provided recursively. Setting this to True means that the child links present in each parent webpage will also be processed. pipeline_batch_size: int Batch size to use for pre-processing stage (Data Processing, Embedding, Vector DB Module). This represents the number of files in each batch. The default value is 20. Note : You may provide both data_dir and web_urls .","title":"Code parameters"},{"location":"tutorials/general/code_parameters/#this-is-a-tutorial-on-the-optional-parameters-that-can-be-passed-into-the-autogluonrag-class-when-using-the-package-through-code","text":"These are the optional parameters that can be passed into the AutoGluonRAG class: config_file : str Path to a configuration file that will be used to set specific parameters in the RAG pipeline. preset_quality : str If you do not wish to use your own configuration file, you can use a preset configuration file which contains pre-defined arguments. You must provide the preset quality setting (\"good_quality\", \"medium_quality\", or, \"best_quality\"). Note that if both config_file and preset_quality are provided, config_file will be prioritized. model_ids : dict Dictionary of model IDs to use for specific modules. Example: {\"generator_model_id\": \"mistral.mistral-7b-instruct-v0:2\", \"retriever_model_id\": \"BAAI/bge-large-en\", \"reranker_model_id\": \"nv_embed\"} data_dir : str The directory containing the data files that will be used for the RAG pipeline. If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. web_urls : List[str] List of website URLs to be ingested and processed. Each URL will processed recursively based on the base URL to include the content of URLs that exist within this URL. If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. base_urls : List[str] List of optional base URLs to check for links recursively. The base URL controls which URLs will be processed during recursion. The base_url does not need to be the same as the web_url. For example. the web_url can be \"https://auto.gluon.ai/stable/index.html\", and the base_urls will be \"https://auto.gluon.ai/stable/\". If this value is not provided when initializing the object, it must be provided in the config file. If both are provided, the value in the class instantiation will be prioritized. login_info: dict A dictionary containing login credentials for each URL. Required if the target URL requires authentication. Must be structured as {target_url: {\"login_url\": <login_url>, \"credentials\": {\"username\": \"your_username\", \"password\": \"your_password\"}}} The target_url is a url that is present in the list of web_urls parse_urls_recursive: bool Whether to parse each URL in the provided recursively. Setting this to True means that the child links present in each parent webpage will also be processed. pipeline_batch_size: int Batch size to use for pre-processing stage (Data Processing, Embedding, Vector DB Module). This represents the number of files in each batch. The default value is 20. Note : You may provide both data_dir and web_urls .","title":"This is a tutorial on the optional parameters that can be passed into the AutoGluonRAG class when using the package through code."},{"location":"tutorials/general/model_access/","text":"This is a tutorial on accessing models through different services in AutoGluonRAG . Depending on what service you are using to access certain models for different modules, you may need to provide access keys for each service. 1. Using GPT models: 1. Create an OpenAI account here: https://openai.com/ 2. Access the API section after logging in. Go to the \u201cAPI\u201d tab or use this link to access the API dashboard 3. Select the appropriate billing plan for your account to access OpenAI models. Complete all necessary financial information and billing steps. 4. Generate and API key and store it in a txt file on your device. When using AutoGluon-RAG, make sure to specify the path to this file by setting the openai_key_file argument in the config file or through code (Refer to Setting Parameters for AutoGluonRAG through code for more info). Using AWS Bedrock models: You can either use the AWS CLI or manually set the AWS Keys in your command line configuration file ( bash_profile or zshrc file). If you are doing it manually, make sure to set the following parameters: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION Using Huggingface models: You can use the Hugging Face Command Line Interface (CLI) to access Hugging Face models. Follow these steps to access Hugging Face models: Installation pip install -U \"huggingface_hub[cli]\" Once installed, check that the CLI is correctly setup: huggingface-cli --help Login to your Hugging Face account First, create a Hugging Face account here . Then, obtain an access token using this link. You can find more information about User access tokens here . Once you have your token, run the following command in your terminal: huggingface-cli login Enter your access token when prompted. You can optionally use the Hugging Face token as a git credential if you plan to use git locally and contribute to this package.","title":"Model access"},{"location":"tutorials/general/model_access/#this-is-a-tutorial-on-accessing-models-through-different-services-in-autogluonrag","text":"Depending on what service you are using to access certain models for different modules, you may need to provide access keys for each service. 1. Using GPT models: 1. Create an OpenAI account here: https://openai.com/ 2. Access the API section after logging in. Go to the \u201cAPI\u201d tab or use this link to access the API dashboard 3. Select the appropriate billing plan for your account to access OpenAI models. Complete all necessary financial information and billing steps. 4. Generate and API key and store it in a txt file on your device. When using AutoGluon-RAG, make sure to specify the path to this file by setting the openai_key_file argument in the config file or through code (Refer to Setting Parameters for AutoGluonRAG through code for more info). Using AWS Bedrock models: You can either use the AWS CLI or manually set the AWS Keys in your command line configuration file ( bash_profile or zshrc file). If you are doing it manually, make sure to set the following parameters: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION Using Huggingface models: You can use the Hugging Face Command Line Interface (CLI) to access Hugging Face models. Follow these steps to access Hugging Face models: Installation pip install -U \"huggingface_hub[cli]\" Once installed, check that the CLI is correctly setup: huggingface-cli --help Login to your Hugging Face account First, create a Hugging Face account here . Then, obtain an access token using this link. You can find more information about User access tokens here . Once you have your token, run the following command in your terminal: huggingface-cli login Enter your access token when prompted. You can optionally use the Hugging Face token as a git credential if you plan to use git locally and contribute to this package.","title":"This is a tutorial on accessing models through different services in AutoGluonRAG."},{"location":"tutorials/general/setting_parameters/","text":"This is a tutorial on setting module parameters in AutoGluonRAG . agrag = AutoGluon() agrag.initialize_rag_pipeline() You can access all the module-level parameters using agrag.args . For example: agrag.args.embedding_model = \"new_model_name\" For specific details about the parameters in each individual module, refer to the README files in each module in src/agrag/modules/ .","title":"Setting parameters"},{"location":"tutorials/general/setting_parameters/#this-is-a-tutorial-on-setting-module-parameters-in-autogluonrag","text":"agrag = AutoGluon() agrag.initialize_rag_pipeline() You can access all the module-level parameters using agrag.args . For example: agrag.args.embedding_model = \"new_model_name\" For specific details about the parameters in each individual module, refer to the README files in each module in src/agrag/modules/ .","title":"This is a tutorial on setting module parameters in AutoGluonRAG."},{"location":"tutorials/generator/change_generator/","text":"This is a tutorial on changing generator after initializing RAG pipeline. agrag = AutoGluon() agrag.initialize_rag_pipeline() # Change Generator Model Configuration agrag.args.generator_model_name = \"new_model\" agrag.args.generator_query_prefix = \"new query prefix\" # Reinitialize Module agrag.initialize_generator_module() response = agrag.generate_response(query_text) Note : You must reinitialize the appropriate module after updating its arguments to propagate the changes into the module.","title":"Change generator"},{"location":"tutorials/generator/change_generator/#this-is-a-tutorial-on-changing-generator-after-initializing-rag-pipeline","text":"agrag = AutoGluon() agrag.initialize_rag_pipeline() # Change Generator Model Configuration agrag.args.generator_model_name = \"new_model\" agrag.args.generator_query_prefix = \"new query prefix\" # Reinitialize Module agrag.initialize_generator_module() response = agrag.generate_response(query_text) Note : You must reinitialize the appropriate module after updating its arguments to propagate the changes into the module.","title":"This is a tutorial on changing generator after initializing RAG pipeline."},{"location":"tutorials/retriever/disable_reranker/","text":"This is a tutorial on disabling the reranker and only use embedding model for retrieval. The reranker is an optional sub-module that can be used within the retriever module to rerank the retrieved text from the Vector DB. There are two ways to configure the reranker. One way is through the config file: retriever: use_reranker: true reranker_model_name: BAAI/bge-large-en reranker_model_platform: huggingface reranker_model_platform_args: null The alternate way is through code: Refer to this tutorial on how to modify arguments through code after instantiating an AutoGluonRAG object. agrag.use_reranker = False agrag.initialize_retriever_module()","title":"Disable reranker"},{"location":"tutorials/retriever/disable_reranker/#this-is-a-tutorial-on-disabling-the-reranker-and-only-use-embedding-model-for-retrieval","text":"The reranker is an optional sub-module that can be used within the retriever module to rerank the retrieved text from the Vector DB. There are two ways to configure the reranker. One way is through the config file: retriever: use_reranker: true reranker_model_name: BAAI/bge-large-en reranker_model_platform: huggingface reranker_model_platform_args: null The alternate way is through code: Refer to this tutorial on how to modify arguments through code after instantiating an AutoGluonRAG object. agrag.use_reranker = False agrag.initialize_retriever_module()","title":"This is a tutorial on disabling the reranker and only use embedding model for retrieval."},{"location":"tutorials/vector_db/optimizing_faiss/","text":"This is a tutorial on optimizing FAISS Vector Database for large scale data. By default, AutoGluon-RAG uses the brute-force version of FAISS - IndexFlatL2 . IndexFlatL2 measures the L2 (or Euclidean) distance between the given query vector and all the vectors stored in the database. While this is highly accurate, it is extremely time-consuming since it has a time-complexity of $O(n)$ (where $n$ is the number of vectors stored in the database). If you have a 1M (million) vectors stored, this would result in 1M comparisons with the query vector. If you have multiple queries, this process would have to be repeated for each one. Thus, it is not the best option for scaling with the size of the data. To solve this, FAISS makes use of two optimized indices: Partitioned/Clustered Index Partitioned indices, also known as clustered indices, use clustering algorithms like k-means to partition the data into multiple clusters. This way, only a subset of clusters needs to be searched, significantly reducing the number of comparisons. Steps to create a Partitioned Index: from faiss import IndexFlatL2, IndexIVFFlat import faiss d = 128 # Dimension of the vectors nlist = 100 # Number of clusters # Initialize the quantizer quantizer = IndexFlatL2(d) # Initialize the IVF index index = IndexIVFFlat(quantizer, d, nlist) # Train the index with your data index.train(vectors) # Assuming 'vectors' is a numpy array of your data # Add vectors to the index index.add(vectors) # Search the index D, I = index.search(query_vectors, k) Usage in AutoGluon-RAG Quantized Index Quantized indices reduce the precision of the vectors to lower the memory footprint and improve search speed. Product Quantization (PQ) is a common technique used in FAISS for this purpose. Steps to create a Quantized Index: from faiss import IndexFlatL2, IndexIVFPQ import faiss d = 128 # Dimension of the vectors nlist = 100 # Number of clusters m = 8 # number of centroid IDs in final compressed vectors bits = 8 # number of bits in each centroid # Initialize the quantizer quantizer = IndexFlatL2(d) # Initialize the IVFPQ index index = IndexIVFPQ(quantizer, d, nlist, m, bits) # Train the index with your data index.train(vectors) # Assuming 'vectors' is a numpy array of your data # Add vectors to the index index.add(vectors) # Search the index D, I = index.search(query_vectors, k) Choosing the Right Index IndexFlatL2 : Use when accuracy is the primary concern, and the dataset is relatively small. IndexIVFFlat : Use when dealing with large datasets and you need to speed up the search process while maintaining reasonable accuracy. IndexPQ : Use when you need to optimize for memory usage and speed at the cost of some precision. Integration into AutoGluon-RAG You must specify the values in your configuration file or after instantiating your AutoGluonRAG object. Refer to this tutorial on how to modify arguments through code after instantiating an AutoGluonRAG object. vector_db: db_type: faiss faiss_index_type: IndexFlatL2, IndexIVFFlat, or IndexIVFPQ faiss_quantized_index_params: Parameters to pass into IndexIVFPQ (d, nlist, m, bits) faiss_clustered_index_params: Parameters to pass into IndexIVFFlat (d, nlist) faiss_index_nprobe: Set nprobe value. This defines how many nearby cells to search. It is applicable for both IndexIVFFlat and IndexIVFPQ","title":"Optimizing faiss"},{"location":"tutorials/vector_db/optimizing_faiss/#this-is-a-tutorial-on-optimizing-faiss-vector-database-for-large-scale-data","text":"By default, AutoGluon-RAG uses the brute-force version of FAISS - IndexFlatL2 . IndexFlatL2 measures the L2 (or Euclidean) distance between the given query vector and all the vectors stored in the database. While this is highly accurate, it is extremely time-consuming since it has a time-complexity of $O(n)$ (where $n$ is the number of vectors stored in the database). If you have a 1M (million) vectors stored, this would result in 1M comparisons with the query vector. If you have multiple queries, this process would have to be repeated for each one. Thus, it is not the best option for scaling with the size of the data. To solve this, FAISS makes use of two optimized indices:","title":"This is a tutorial on optimizing FAISS Vector Database for large scale data."},{"location":"tutorials/vector_db/optimizing_faiss/#partitionedclustered-index","text":"Partitioned indices, also known as clustered indices, use clustering algorithms like k-means to partition the data into multiple clusters. This way, only a subset of clusters needs to be searched, significantly reducing the number of comparisons.","title":"Partitioned/Clustered Index"},{"location":"tutorials/vector_db/optimizing_faiss/#steps-to-create-a-partitioned-index","text":"from faiss import IndexFlatL2, IndexIVFFlat import faiss d = 128 # Dimension of the vectors nlist = 100 # Number of clusters # Initialize the quantizer quantizer = IndexFlatL2(d) # Initialize the IVF index index = IndexIVFFlat(quantizer, d, nlist) # Train the index with your data index.train(vectors) # Assuming 'vectors' is a numpy array of your data # Add vectors to the index index.add(vectors) # Search the index D, I = index.search(query_vectors, k)","title":"Steps to create a Partitioned Index:"},{"location":"tutorials/vector_db/optimizing_faiss/#usage-in-autogluon-rag","text":"","title":"Usage in AutoGluon-RAG"},{"location":"tutorials/vector_db/optimizing_faiss/#quantized-index","text":"Quantized indices reduce the precision of the vectors to lower the memory footprint and improve search speed. Product Quantization (PQ) is a common technique used in FAISS for this purpose.","title":"Quantized Index"},{"location":"tutorials/vector_db/optimizing_faiss/#steps-to-create-a-quantized-index","text":"from faiss import IndexFlatL2, IndexIVFPQ import faiss d = 128 # Dimension of the vectors nlist = 100 # Number of clusters m = 8 # number of centroid IDs in final compressed vectors bits = 8 # number of bits in each centroid # Initialize the quantizer quantizer = IndexFlatL2(d) # Initialize the IVFPQ index index = IndexIVFPQ(quantizer, d, nlist, m, bits) # Train the index with your data index.train(vectors) # Assuming 'vectors' is a numpy array of your data # Add vectors to the index index.add(vectors) # Search the index D, I = index.search(query_vectors, k)","title":"Steps to create a Quantized Index:"},{"location":"tutorials/vector_db/optimizing_faiss/#choosing-the-right-index","text":"IndexFlatL2 : Use when accuracy is the primary concern, and the dataset is relatively small. IndexIVFFlat : Use when dealing with large datasets and you need to speed up the search process while maintaining reasonable accuracy. IndexPQ : Use when you need to optimize for memory usage and speed at the cost of some precision.","title":"Choosing the Right Index"},{"location":"tutorials/vector_db/optimizing_faiss/#integration-into-autogluon-rag","text":"You must specify the values in your configuration file or after instantiating your AutoGluonRAG object. Refer to this tutorial on how to modify arguments through code after instantiating an AutoGluonRAG object. vector_db: db_type: faiss faiss_index_type: IndexFlatL2, IndexIVFFlat, or IndexIVFPQ faiss_quantized_index_params: Parameters to pass into IndexIVFPQ (d, nlist, m, bits) faiss_clustered_index_params: Parameters to pass into IndexIVFFlat (d, nlist) faiss_index_nprobe: Set nprobe value. This defines how many nearby cells to search. It is applicable for both IndexIVFFlat and IndexIVFPQ","title":"Integration into AutoGluon-RAG"}]}